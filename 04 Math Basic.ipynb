{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["## Optimization\n","\n","#### 1. Simple gradient descent method\n","- $x \\leftarrow x-\\alpha f'(x)$  \n"," where $f(x)$ is a convex function.\n","- Learning rate effect\n","- Initial point effect\n","- Local minima\n","- Newton-Raphson method\n"," - $x \\leftarrow x - {{f'(x)}\\over{f''(x)}}$\n","\n","\n","#### 2. Multi-variable gradient descent method\n","- $\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\alpha {f'(\\boldsymbol{x})}$\n","\n"," where $\\boldsymbol{x} = \\begin{bmatrix}\n","  x_1 \\\\\n","  x_2 \\\\\n","  \\vdots \\\\\n","\\end{bmatrix}$  \n","(bold notation)"],"metadata":{"id":"j1vlbc8PDAKJ"}},{"cell_type":"markdown","metadata":{"id":"L6KaEKbNvS1A"},"source":["#### 1. Simple gradient descent method\n","- $x \\leftarrow x-\\alpha f'(x)$  \n"," where $f(x)$ is a convex function.\n","\n"," e.g.1) $y=f(x)=x^2-6x+8$"]},{"cell_type":"code","metadata":{"id":"p-_zz1dzvAfN"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","x = np.linspace(0,5)\n","y = (x-2)*(x-4)\n","\n","plt.plot(x,y)\n","plt.plot(3,-1,'r*',label='Minimum')\n","plt.grid()\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOqWwq6pvGXx"},"source":["def func(x):\n","  return (x-2)*(x-4) # objective function\n","\n","def grad(x):\n","  return 2*x-6 # gradient\n","\n","x_init = 0\n","y_init = func(x_init) # initialization (0, f(0)); x_0\n","\n","plt.plot(x,y)\n","plt.plot(x_init,y_init,'r*',label='0 times point') # initial point plotting\n","\n","alpha = 0.2 # learning rate\n","iter = 13 # total iteration\n","k = 2\n","\n","for ep in range(iter):\n","  x_init -= alpha*grad(x_init)\n","  if ep % k == 0:\n","    plt.plot(x_init,func(x_init),'*',label='{:d} times point'.format(ep+1))\n","\n","print(x_init,func(x_init)) # solution from numerial method\n","\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyPDnfnswyKy"},"source":["- Learning rate effect\n"]},{"cell_type":"code","metadata":{"id":"A7e_m5Mvw5Am"},"source":["x_init = 0\n","y_init = func(x_init)\n","\n","plt.plot(x,y)\n","plt.plot(x_init,y_init,'r*',label='0 times point')\n","\n","alpha = 0.1 # learning rate\n","iter = 13 # total iteration\n","k = 2\n","\n","for ep in range(iter):\n","  x_init -= alpha*grad(x_init)\n","  if ep % k == 0:\n","    plt.plot(x_init,func(x_init),'*',label='{:d} times point'.format(ep+1))\n","\n","print(x_init,func(x_init)) # solution from numerial method\n","\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Initial point effect"],"metadata":{"id":"YL8nT37HOgy8"}},{"cell_type":"code","source":["x_init = 2\n","y_init = func(x_init)\n","\n","plt.plot(x,y)\n","plt.plot(x_init,y_init,'r*',label='0 times point')\n","\n","alpha = 0.1 # learning rate\n","iter = 13 # total iteration\n","k = 2\n","\n","for ep in range(iter):\n","  x_init -= alpha*grad(x_init)\n","  if ep % k == 0:\n","    plt.plot(x_init,func(x_init),'*',label='{:d} times point'.format(ep+1))\n","\n","print(x_init,func(x_init)) # solution from numerial method\n","\n","plt.legend()\n","plt.grid()"],"metadata":{"id":"zG7Tc8vxOY57"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mOS8r21tvIS_"},"source":["- Local minima\n","\n"," e.g.2) 4th-order function"]},{"cell_type":"code","metadata":{"id":"wjhE6BC6vRAw"},"source":["def func(x):\n","    return (x-2.45)*(x-0.06)*(x-1.35)*(x-4.5) # two local minima\n","\n","def grad(x):\n","    return (func(x+1e-10)-func(x))/1e-10\n","\n","y = func(x)\n","plt.plot(x,y)\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Or4OaRjRwFm_"},"source":["# local minima\n","x_init = 0\n","y_init = func(x_init)\n","\n","plt.plot(x,y)\n","plt.plot(x_init,y_init,'r*',label='0 times point')\n","\n","alpha = 0.01 # learning rate\n","iter = 10 # total iteration\n","k = 3 # plotting point\n","\n","for ep in range(iter):\n","  x_init -= alpha*grad(x_init)\n","  if ep % k == 0:\n","    plt.plot(x_init,func(x_init),'*',label='{:d} times point'.format(ep+1))\n","\n","print(x_init,func(x_init)) # solution from numerial method\n","\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1yfJ4C_xrqJ"},"source":["# next initialize\n","x_init = 5\n","y_init = func(x_init)\n","\n","plt.plot(x,y)\n","plt.plot(x_init,y_init,'r*',label='0 times point')\n","\n","alpha = 0.01 # learning rate\n","iter = 45 # total iteration\n","k = 3\n","\n","for ep in range(iter):\n","  x_init -= alpha*grad(x_init)\n","  if ep % k == 0:\n","    plt.plot(x_init,func(x_init),'*',label='{:d} times point'.format(ep+1))\n","\n","print(x_init,func(x_init)) # solution from numerial method\n","\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Multi-variable gradient descent method\n","- $\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\alpha {f'(\\boldsymbol{x})}$\n","\n"," where $\\boldsymbol{x} = \\begin{bmatrix}\n","  x_1 \\\\\n","  x_2 \\\\\n","  \\vdots \\\\\n","\\end{bmatrix}$  \n","(bold notation)"],"metadata":{"id":"MN9FyjpkAulE"}},{"cell_type":"code","source":["# Stochastic gradient descent for 2-dimensional data\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","x1 = np.linspace(-3,3)\n","x2 = np.linspace(-3,3)\n","print(x1)\n","print(x1.shape)\n","X1,X2 = np.meshgrid(x1,x2)\n","\n","def test_func(X1,X2):\n","  return X1**2+X2**2 # (x1,x2) = (0,0) minimum\n","\n","def grad_test(X1,X2):\n","    return np.array(2*X1, 2*X2)"],"metadata":{"id":"wTYglC7mAzcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([-1.78,-1.85])\n","x1,x2 = X\n","Z = test_func(X1,X2)\n","plt.contour(X1,X2,Z)\n","plt.plot(x1,x2,'ro')\n","plt.colorbar()"],"metadata":{"id":"aHP61oLeA0K9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans_buff = []\n","for iter in range(10):\n","  ans_buff.append(X)\n","  print(X)\n","  X = X - 0.1*grad_test(X[0],X[1])"],"metadata":{"id":"1AWAc8NKA3Kb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans_buff = np.array(ans_buff)\n","plt.contour(X1,X2,Z)\n","plt.plot(ans_buff[:,0],ans_buff[:,1],'ro-')\n","plt.colorbar()"],"metadata":{"id":"CFlFE_aHA46f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ans_buff[-1]) # solution from stochastic gadient descent"],"metadata":{"id":"Uui7amSrA6j0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q: Implement the gradient descent algorithm using partial derivatives of the parameters."],"metadata":{"id":"MJcoA792FDz8"}}]}